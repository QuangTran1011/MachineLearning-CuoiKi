import re
from datetime import datetime, timedelta

from airflow import DAG
from airflow.providers.google.cloud.operators.gcs import GCSListObjectsOperator
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.kubernetes.secret import Secret

# ----------------------------
# Config
# ----------------------------
BUCKET = "kltn--data"
PREFIX = "partitiondata/"
FILE_PATTERN = r"recsys_data_upto_(\d{4})_(\d{2})_(\d{2})\.parquet"
IMAGE = "quangtran1011/airflow_all_in_one:v4"

default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

# Secret mount for GCP service account
gcp_sa_secret = Secret(
    deploy_type='volume',            # mount as volume
    deploy_target='/var/secrets/google',  # mount path in container (was 'mount_point')
    secret='gcp-sa-secret',             # name of K8s secret
    key='gcp-key.json'               # key file name inside secret
)
env_sa = {"GOOGLE_APPLICATION_CREDENTIALS": "/var/secrets/google/gcp-key.json"}

# ----------------------------
# DAG
# ----------------------------
with DAG(
    "gcs_trigger_pipeline",
    default_args=default_args,
    schedule_interval="0 1 * * *",
    start_date=datetime(2025, 11, 26),
    catchup=False,
) as dag:

    # 1. List file parquet trên GCS
    list_files = GCSListObjectsOperator(
        task_id="list_files",
        bucket=BUCKET,
        prefix=PREFIX,
        gcp_conn_id='google_cloud_default',  # sẽ dùng GOOGLE_APPLICATION_CREDENTIALS
        do_xcom_push=True,
    )

    # 2. Check file mới nhất xem có phải ngày hôm nay không
    def check_file_date(**context):
        files = context["ti"].xcom_pull(task_ids="list_files")
        if not files:
            print("Không có file parquet.")
            return False
        parquet_files = [f for f in files if f.endswith(".parquet")]
        if not parquet_files:
            print("Không tìm thấy file parquet.")
            return False
        latest_file = sorted(parquet_files)[-1]
        print("File detect:", latest_file)
        match = re.search(FILE_PATTERN, latest_file)
        if not match:
            print("Không match tên file.")
            return False
        yyyy, mm, dd = match.groups()
        file_date = datetime(int(yyyy), int(mm), int(dd)).date()
        today = datetime.utcnow().date()
        print("File date:", file_date)
        print("Today:", today)
        return file_date == today

    check_file = ShortCircuitOperator(
        task_id="check_file",
        python_callable=check_file_date,
    )

    # ----------------------------
    # 3. Spark job 1
    # ----------------------------
    spark_job_1 = KubernetesPodOperator(
        task_id="spark_job_1",
        name="spark-job-1",
        namespace="serving",
        image=IMAGE,
        cmds=["/opt/spark/bin/spark-submit"],
        arguments=[
            "--master", "k8s://https://34.118.224.1:443",
            "--deploy-mode", "cluster",
            "--conf", "spark.kubernetes.namespace=serving",
            "--conf", f"spark.kubernetes.container.image={IMAGE}", 
            "--conf", "spark.kubernetes.authenticate.driver.serviceAccountName=default",
            "--conf", "spark.kubernetes.driver.secrets.gcp-sa-secret=/var/secrets/google",  
            "--conf", "spark.kubernetes.executor.secrets.gcp-sa-secret=/var/secrets/google",
            "--conf", "spark.kubernetes.driverEnv.PYTHONPATH=/app:/app/modules",
            "--conf", "spark.kubernetes.executorEnv.PYTHONPATH=/app:/app/modules",
            
            # Set env var cho driver và executor
            "--conf", "spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/google/gcp-key.json",
            "--conf", "spark.kubernetes.executorEnv.GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/google/gcp-key.json",
            # Giảm CPU
            "--conf", "spark.executor.memory=400m", 
            "--conf", "spark.driver.memory=512m",
            
            # Giảm memory overhead
            "--conf", "spark.kubernetes.memoryOverheadFactor=0.1",  
            
            # Resource requests/limits thấp để K8s có thể schedule
            "--conf", "spark.kubernetes.driver.request.cores=250m",
            "--conf", "spark.kubernetes.executor.request.cores=250m",
            "--conf", "spark.kubernetes.driver.request.memory=512Mi",  
            "--conf", "spark.kubernetes.executor.request.memory=512Mi",
            
            # Tùy chọn thêm (limits)
            "--conf", "spark.kubernetes.driver.limit.memory=600Mi",
            "--conf", "spark.kubernetes.executor.limit.memory=600Mi",
            "local:///app/spark_job/sampling.py",
        ],
        get_logs=True,
        is_delete_operator_pod=True,
        secrets=[gcp_sa_secret],
        env_vars=env_sa,
    )

    # Spark job 2
    spark_job_2 = KubernetesPodOperator(
        task_id="spark_job_2",
        name="spark-job-2",
        namespace="serving",
        image=IMAGE,
        cmds=["/opt/spark/bin/spark-submit"],
        arguments=[
            "--master", "k8s://https://34.118.224.1:443",
            "--deploy-mode", "cluster",
            "--conf", "spark.kubernetes.namespace=serving",
            "--conf", f"spark.kubernetes.container.image={IMAGE}",  
            "--conf", "spark.kubernetes.authenticate.driver.serviceAccountName=default",
            "--conf", "spark.kubernetes.driver.secrets.gcp-sa-secret=/var/secrets/google", 
            "--conf", "spark.kubernetes.executor.secrets.gcp-sa-secret=/var/secrets/google",
            "--conf", "spark.kubernetes.driverEnv.PYTHONPATH=/app:/app/modules",
            "--conf", "spark.kubernetes.executorEnv.PYTHONPATH=/app:/app/modules",
            
            # Set env var cho driver và executor
            "--conf", "spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/google/gcp-key.json",
            "--conf", "spark.kubernetes.executorEnv.GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/google/gcp-key.json",
            # Giảm CPU
            "--conf", "spark.driver.cores=1",
            "--conf", "spark.executor.cores=1",
            
            # Giảm memory xuống 512MB (phải để thấp hơn chút để có overhead)
            "--conf", "spark.executor.memory=400m", 
            "--conf", "spark.driver.memory=512m",
            
            # Giảm memory overhead
            "--conf", "spark.kubernetes.memoryOverheadFactor=0.1",  
            
            # Resource requests/limits thấp để K8s có thể schedule
            "--conf", "spark.kubernetes.driver.request.cores=250m",
            "--conf", "spark.kubernetes.executor.request.cores=250m",
            "--conf", "spark.kubernetes.driver.request.memory=512Mi",  
            "--conf", "spark.kubernetes.executor.request.memory=512Mi",
            
            # Tùy chọn thêm (limits)
            "--conf", "spark.kubernetes.driver.limit.memory=600Mi",
            "--conf", "spark.kubernetes.executor.limit.memory=600Mi",
            "local:///app/spark_job/sampling_item.py",
        ],
        get_logs=True,
        is_delete_operator_pod=True,
        secrets=[gcp_sa_secret],
        env_vars=env_sa,
    )


    upload_to_dbms = KubernetesPodOperator(
        task_id="upload_to_dbms",
        name="upload-to-dbms",
        namespace="serving",
        image=IMAGE,
        cmds=["python3"],
        arguments=["/app/spark_job/upload_to_dbms.py"],
        get_logs=True,
        is_delete_operator_pod=True,
        secrets=[gcp_sa_secret],
        env_vars=env_sa,
    )

    # ----------------------------
    # 5. dbt build
    # ----------------------------
    dbt_build = KubernetesPodOperator(
        task_id="dbt_build",
        name="dbt-build",
        namespace="serving",
        image=IMAGE,
        cmds=["dbt"],
        arguments=["build", "--project-dir", "/app/dbt_project/kltn"],
        get_logs=True,
        is_delete_operator_pod=True,
        secrets=[gcp_sa_secret],
        env_vars=env_sa,
    )

    # ----------------------------
    # 6. Feast materialize incremental
    # ----------------------------
    feast_run = KubernetesPodOperator(
        task_id="feast_materialize",
        name="feast-materialize",
        namespace="serving",
        image=IMAGE,
        cmds=["feast"],
        arguments=[
            "materialize-incremental",
            str(datetime.utcnow().date()),
            "--feature-store", "/app/feature_repo/feature_store.yaml",
        ],
        get_logs=True,
        is_delete_operator_pod=True,
        secrets=[gcp_sa_secret],
        env_vars=env_sa,
    )

    # ----------------------------
    # Pipeline
    # ----------------------------
    list_files >> check_file >> spark_job_1 >> upload_to_dbms >> dbt_build >> feast_run >> spark_job_2
